{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca3df69",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-14T10:52:27.967321Z",
     "iopub.status.busy": "2022-11-14T10:52:27.966843Z",
     "iopub.status.idle": "2022-11-14T10:52:38.604374Z",
     "shell.execute_reply": "2022-11-14T10:52:38.599677Z"
    },
    "papermill": {
     "duration": 10.645538,
     "end_time": "2022-11-14T10:52:38.608387",
     "exception": false,
     "start_time": "2022-11-14T10:52:27.962849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pneumonia Detection using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991c6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input,Lambda , Dense , Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40f0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ff88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50c53b",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ecc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224,224]\n",
    "\n",
    "train_path =\"Datasets/train\"\n",
    "test_path = \"Datasets/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ba2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40dc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers : \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83bb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob('Datasets/train/*')\n",
    "x = Flatten() (vgg.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68f5958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction = Dense(len(folders),activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs = prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a905f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "loss='categorical_crossentropy',\n",
    "optimizer = 'adam',\n",
    "metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33edb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8992b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip= True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f5547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8960f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('Datasets/train',\n",
    "                                                target_size = (224,224),\n",
    "                                                batch_size = 10,\n",
    "                                                class_mode ='categorical') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6fe53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('Datasets/test',\n",
    "                                           target_size= (224,224),\n",
    "                                            batch_size = 10,\n",
    "                                            class_mode ='categorical'\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84c8d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hepha\\AppData\\Local\\Temp\\ipykernel_19180\\3779747041.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1010s 2s/step - loss: 0.2026 - accuracy: 0.9256 - val_loss: 0.2931 - val_accuracy: 0.9135\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "training_set,\n",
    "validation_data = test_set,\n",
    "epochs = 1 ,\n",
    "steps_per_epoch = len(training_set),\n",
    "validation_steps = len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7261a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('chest_xray.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7df685",
   "metadata": {},
   "source": [
    "## Import model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cde2445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('chest_xray.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd9cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e7c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0674cba",
   "metadata": {},
   "source": [
    "## Normal xray images : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image.load_img doesnt work so i had to use tf.keras.utils\n",
    "img = tf.keras.utils.load_img('./Datasets/val/NORMAL/NORMAL2-IM-1430-0001.jpeg',target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f0d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.keras.utils.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595ec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a158bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e393ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "classes= model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa61e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c261e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result is Normal\n"
     ]
    }
   ],
   "source": [
    "if result > 0.9 : \n",
    "    print(\"Result is Normal\")\n",
    "else:\n",
    "    print(\"Person is Affected by pneumonia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db5a2f",
   "metadata": {},
   "source": [
    "## pneumonic xray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce2ba761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image.load_img doesnt work so i had to use tf.keras.utils\n",
    "img = tf.keras.utils.load_img('./Datasets/val/PNEUMONIA/person1946_bacteria_4875.jpeg',target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2fbfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.keras.utils.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ccb2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ddb155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aab8b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 153ms/step\n"
     ]
    }
   ],
   "source": [
    "classes= model.predict(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd789177",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classes[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68b3d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person is Affected by pneumonia\n"
     ]
    }
   ],
   "source": [
    "if result > 0.9 : \n",
    "    print(\"Result is Normal\")\n",
    "else:\n",
    "    print(\"Person is Affected by pneumonia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4c740",
   "metadata": {},
   "source": [
    "## Accuracy : 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b46c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.049577,
   "end_time": "2022-11-14T10:52:39.267825",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-14T10:52:19.218248",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
